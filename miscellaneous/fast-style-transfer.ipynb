{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class instance_norm(tf.keras.layers.Layer):\n",
    "    def __init__(self, epsilon=1e-3):\n",
    "        super(instance_norm, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.beta = tf.Variable(tf.zeros([input_shape[3]]))\n",
    "        self.gamma = tf.Variable(tf.ones([input_shape[3]]))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mean, var = tf.nn.moments(inputs, axes=[1, 2], keepdims=True)\n",
    "        x = tf.divide(tf.subtract(inputs, mean), tf.sqrt(tf.add(var, self.epsilon)))\n",
    "        \n",
    "        return self.gamma * x + self.beta\n",
    "\n",
    "class conv_2d(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, kernel, stride):\n",
    "        super(conv_2d, self).__init__()\n",
    "        pad = kernel // 2\n",
    "        self.paddings = tf.constant([[0, 0], [pad, pad],[pad, pad], [0, 0]])\n",
    "        self.conv2d = tf.keras.layers.Conv2D(filters, kernel, stride, use_bias=False, padding='valid')\n",
    "        self.instance_norm = instance_norm()\n",
    "\n",
    "    def call(self, inputs, relu=True):\n",
    "        x = tf.pad(inputs, self.paddings, mode='REFLECT')\n",
    "        x = self.conv2d(x)\n",
    "        x = self.instance_norm(x)\n",
    "\n",
    "        if relu:\n",
    "            x = tf.nn.relu(x)\n",
    "        return x\n",
    "\n",
    "class resize_conv_2d(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, kernel, stride):\n",
    "        super(resize_conv_2d, self).__init__()\n",
    "        self.conv = conv_2d(filters, kernel, stride)\n",
    "        self.instance_norm = instance_norm()\n",
    "        self.stride = stride\n",
    "\n",
    "    def call(self, inputs):\n",
    "        new_h = inputs.shape[1] * self.stride * 2\n",
    "        new_w = inputs.shape[2] * self.stride * 2\n",
    "        x = tf.image.resize(inputs, [new_h, new_w], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        x = self.conv(x)\n",
    "        # return x\n",
    "\n",
    "        \"\"\" Redundant \"\"\"\n",
    "        x = self.instance_norm(x)\n",
    "\n",
    "        return tf.nn.relu(x)\n",
    "\n",
    "class tran_conv_2d(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, kernel, stride):\n",
    "        super(tran_conv_2d, self).__init__()\n",
    "        self.tran_conv = tf.keras.layers.Conv2DTranspose(filters, kernel, stride, padding='same')\n",
    "        self.instance_norm = instance_norm()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.tran_conv(inputs)\n",
    "        x = self.instance_norm(x)\n",
    "\n",
    "        return tf.nn.relu(x)\n",
    "\n",
    "class residual(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, kernel, stride):\n",
    "        super(residual, self).__init__()\n",
    "        self.conv1 = conv_2d(filters, kernel, stride)\n",
    "        self.conv2 = conv_2d(filters, kernel, stride)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        return inputs + self.conv2(x, relu=False)\n",
    "        \n",
    "\n",
    "class feed_forward(tf.keras.models.Model):\n",
    "    def __init__(self):\n",
    "        super(feed_forward, self).__init__()\n",
    "        # [filters, kernel, stride]\n",
    "        self.conv1 = conv_2d(32, 9, 1)     \n",
    "        self.conv2 = conv_2d(64, 3, 2)           \n",
    "        self.conv3 = conv_2d(128, 3, 2)     \n",
    "        self.resid1 = residual(128, 3, 1)         \n",
    "        self.resid2 = residual(128, 3, 1)          \n",
    "        self.resid3 = residual(128, 3, 1)     \n",
    "        self.resid4 = residual(128, 3, 1)     \n",
    "        self.resid5 = residual(128, 3, 1)    \n",
    "        #self.tran_conv1 = tran_conv_2d(64, 3, 2)  \n",
    "        #self.tran_conv2 = tran_conv_2d(32, 3, 2)    \n",
    "        self.resize_conv1 = resize_conv_2d(64, 3, 2)\n",
    "        self.resize_conv2 = resize_conv_2d(32, 3, 2)\n",
    "        self.conv4 = conv_2d(3, 9, 1)              \n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.resid1(x)\n",
    "        x = self.resid2(x)\n",
    "        x = self.resid3(x)\n",
    "        x = self.resid4(x)\n",
    "        x = self.resid5(x)\n",
    "        x = self.resize_conv1(x)\n",
    "        x = self.resize_conv2(x)\n",
    "        x = self.conv4(x, relu=False)\n",
    "        return (tf.nn.tanh(x) * 150 + 255. / 2)     # for better convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "\n",
    "def tensor_to_image(tensor):\n",
    "    tensor = np.array(tensor, dtype=np.uint8)\n",
    "    if np.ndim(tensor)>3:\n",
    "        assert tensor.shape[0] == 1\n",
    "        tensor = tensor[0]\n",
    "    return PIL.Image.fromarray(tensor)\n",
    "    \n",
    "\n",
    "def load_img(path_to_img, max_dim=None, resize=True):\n",
    "    img = tf.io.read_file(path_to_img)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    \n",
    "    if resize:\n",
    "        new_shape = tf.cast([256, 256], tf.int32)\n",
    "        img = tf.image.resize(img, new_shape)\n",
    "\n",
    "    if max_dim:\n",
    "        shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
    "        long_dim = max(shape)\n",
    "        scale = max_dim / long_dim\n",
    "        new_shape = tf.cast(shape * scale, tf.int32)\n",
    "        img = tf.image.resize(img, new_shape)\n",
    "        \n",
    "    img = img[tf.newaxis, :]\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def resolve_video(network, path_to_video, result):\n",
    "    cap = cv2.VideoCapture(path_to_video)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter(result, fourcc, 30.0, (640,640))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        #frame = cv2.resize(frame, (256, 256), interpolation = cv2.INTER_LINEAR) \n",
    "\n",
    "        print('Transfering video...')\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = tf.cast(frame[tf.newaxis, ...], tf.float32) / 255.0\n",
    "\n",
    "        prediction = network(frame)\n",
    "\n",
    "        prediction = clip_0_1(prediction)\n",
    "        prediction = np.array(prediction).astype(np.uint8).squeeze()\n",
    "        prediction = cv2.cvtColor(prediction, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        out.write(prediction)\n",
    "        cv2.imshow('prediction', prediction)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyALLWindow()\n",
    "\n",
    "\n",
    "def create_folder(diirname):\n",
    "    if not os.path.exists(diirname):\n",
    "        os.mkdir(diirname)\n",
    "        print('Directory ', diirname, ' createrd')\n",
    "    else:\n",
    "        print('Directory ', diirname, ' already exists')       \n",
    "\n",
    "\n",
    "def clip_0_1(image):\n",
    "    return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_layers(layer_names):\n",
    "    vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "    vgg.trainable = False\n",
    "\n",
    "    outputs = [vgg.get_layer(name).output for name in layer_names]\n",
    "\n",
    "    model = tf.keras.Model([vgg.input], outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(features, normalize=True):\n",
    "    batch_size, height, width, filters = features.shape\n",
    "    features = tf.reshape(features, (batch_size, height * width, filters))\n",
    "\n",
    "    tran_f = tf.transpose(features, perm=[0, 2, 1])\n",
    "    gram = tf.matmul(features, features)\n",
    "    if normalize:\n",
    "        gram = gram / tf.cast(height * width, tf.float32)\n",
    "    \n",
    "    return gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_loss(style_outputs, style_target):\n",
    "    style_loss = tf.add_n([tf.reduce_mean((style_outputs[name] - style_target[name]) ** 2) for name in style_outputs.keys()])\n",
    "    return style_loss / len(style_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(content_outputs, content_target):\n",
    "    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name] - content_target[name]) ** 2) for name in content_outputs.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation_loss(image):\n",
    "    x_var = image[:,:,1:,:] - image[:,:,:-1,:]\n",
    "    y_var = image[:,1:,:,:] - image[:,:-1,:,:]\n",
    "\n",
    "    return tf.reduce_mean(tf.square(x_var)) + tf.reduce_mean(tf.square(y_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleContentModel(tf.keras.models.Model):\n",
    "    def __init__(self, style_layers, content_layers):\n",
    "        super(StyleContentModel, self).__init__()\n",
    "        self.vgg = vgg_layers(style_layers + content_layers)\n",
    "        self.style_layers = style_layers\n",
    "        self.content_layers = content_layers\n",
    "        self.num_style_layers = len(style_layers)\n",
    "        self.vgg.trainable = False\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        preprocessed_input = preprocessed_input(inputs)\n",
    "        outputs = self.vgg(preprocessed_input)\n",
    "        style_outputs, content_outputs = (outputs[:self.num_style_layers], outputs[self.num_style_layers:])\n",
    "\n",
    "        style_outputs = [gram_matrix(style_output) for style_output in style_outputs]\n",
    "\n",
    "        style_dict = {style_layer: value for style_layer, value in zip(self.style_layers, style_outputs)}\n",
    "        content_dict = {content_layer: value for content_layer, value in zip(self.content_layers, content_outputs)}\n",
    "\n",
    "        return {'style': style_dict, 'content': content_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"VGG19 model for Keras.\n",
    "# Reference\n",
    "- [Very Deep Convolutional Networks for Large-Scale Image Recognition](\n",
    "    https://arxiv.org/abs/1409.1556) (ICLR 2015)\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "backend = tf.keras.backend\n",
    "layers = tf.keras.layers\n",
    "models = tf.keras.models\n",
    "keras_utils = tf.keras.utils\n",
    "\n",
    "WEIGHTS_PATH = ('https://github.com/fchollet/deep-learning-models/'\n",
    "                'releases/download/v0.1/'\n",
    "                'vgg19_weights_tf_dim_ordering_tf_kernels.h5')\n",
    "WEIGHTS_PATH_NO_TOP = ('https://github.com/fchollet/deep-learning-models/'\n",
    "                       'releases/download/v0.1/'\n",
    "                       'vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "\n",
    "_IMAGENET_MEAN = None\n",
    "\n",
    "\n",
    "def VGG19(include_top=True,\n",
    "          weights='imagenet',\n",
    "          input_tensor=None,\n",
    "          input_shape=None,\n",
    "          pooling=None,\n",
    "          classes=1000,\n",
    "          **kwargs):\n",
    "\n",
    "    if not (weights in {'imagenet', None} or os.path.exists(weights)):\n",
    "        raise ValueError('The `weights` argument should be either '\n",
    "                         '`None` (random initialization), `imagenet` '\n",
    "                         '(pre-training on ImageNet), '\n",
    "                         'or the path to the weights file to be loaded.')\n",
    "\n",
    "    if weights == 'imagenet' and include_top and classes != 1000:\n",
    "        raise ValueError('If using `weights` as `\"imagenet\"` with `include_top`'\n",
    "                         ' as true, `classes` should be 1000')\n",
    "\n",
    "    # Determine proper input shape\n",
    "    input_shape = _obtain_input_shape(input_shape,\n",
    "                                      default_size=224,\n",
    "                                      min_size=32,\n",
    "                                      require_flatten=include_top,\n",
    "                                      weights=weights)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = layers.Input(shape=input_shape)\n",
    "    else:\n",
    "        if not backend.is_keras_tensor(input_tensor):\n",
    "            img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "    # Block 1\n",
    "    x = layers.Conv2D(64, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block1_conv1')(img_input)\n",
    "    x = layers.Conv2D(64, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block1_conv2')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = layers.Conv2D(128, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block2_conv1')(x)\n",
    "    x = layers.Conv2D(128, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block2_conv2')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = layers.Conv2D(256, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block3_conv1')(x)\n",
    "    x = layers.Conv2D(256, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block3_conv2')(x)\n",
    "    x = layers.Conv2D(256, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block3_conv3')(x)\n",
    "    x = layers.Conv2D(256, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block3_conv4')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block4_conv1')(x)\n",
    "    x = layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block4_conv2')(x)\n",
    "    x = layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block4_conv3')(x)\n",
    "    x = layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block4_conv4')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block5_conv1')(x)\n",
    "    x = layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block5_conv2')(x)\n",
    "    x = layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block5_conv3')(x)\n",
    "    x = layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block5_conv4')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
    "\n",
    "    if include_top:\n",
    "        # Classification block\n",
    "        x = layers.Flatten(name='flatten')(x)\n",
    "        x = layers.Dense(4096, activation='relu', name='fc1')(x)\n",
    "        x = layers.Dense(4096, activation='relu', name='fc2')(x)\n",
    "        x = layers.Dense(classes, activation='softmax', name='predictions')(x)\n",
    "    else:\n",
    "        if pooling == 'avg':\n",
    "            x = layers.GlobalAveragePooling2D()(x)\n",
    "        elif pooling == 'max':\n",
    "            x = layers.GlobalMaxPooling2D()(x)\n",
    "\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = keras_utils.get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "\n",
    "    # Create model.\n",
    "    model = models.Model(inputs, x, name='vgg19')\n",
    "\n",
    "    # Load weights.\n",
    "    if weights == 'imagenet':\n",
    "        if include_top:\n",
    "            weights_path = keras_utils.get_file(\n",
    "                'vgg19_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                WEIGHTS_PATH,\n",
    "                cache_subdir='models',\n",
    "                file_hash='cbe5617147190e668d6c5d5026f83318')\n",
    "        else:\n",
    "            weights_path = keras_utils.get_file(\n",
    "                'vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                WEIGHTS_PATH_NO_TOP,\n",
    "                cache_subdir='models',\n",
    "                file_hash='253f8cb515780f3b799900260a226db6')\n",
    "        model.load_weights(weights_path)\n",
    "        if backend.backend() == 'theano':\n",
    "            keras_utils.convert_all_kernels_in_model(model)\n",
    "    elif weights is not None:\n",
    "        model.load_weights(weights)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def preprocess_input(x):\n",
    "    \"\"\"Preprocesses a tensor or Numpy array encoding a batch of images.\n",
    "    # Arguments\n",
    "        x: Input Numpy or symbolic tensor, 3D or 4D.\n",
    "            The preprocessed data is written over the input data\n",
    "            if the data types are compatible. To avoid this\n",
    "            behaviour, `numpy.copy(x)` can be used.\n",
    "        data_format: Data format of the image tensor/array.\n",
    "        mode: One of \"caffe\", \"tf\" or \"torch\".\n",
    "            - caffe: will convert the images from RGB to BGR,\n",
    "                then will zero-center each color channel with\n",
    "                respect to the ImageNet dataset,\n",
    "                without scaling.\n",
    "            - tf: will scale pixels between -1 and 1,\n",
    "                sample-wise.\n",
    "            - torch: will scale pixels between 0 and 1 and then\n",
    "                will normalize each channel with respect to the\n",
    "                ImageNet dataset.\n",
    "    # Returns\n",
    "        Preprocessed tensor or Numpy array.\n",
    "    # Raises\n",
    "        ValueError: In case of unknown `data_format` argument.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    mean = np.array([123.68, 116.779, 103.939])\n",
    "        \n",
    "    return (x - mean)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _obtain_input_shape(input_shape,\n",
    "                        default_size,\n",
    "                        min_size,\n",
    "                        require_flatten,\n",
    "                        weights=None):\n",
    "    \"\"\"Internal utility to compute/validate a model's input shape.\n",
    "    # Arguments\n",
    "        input_shape: Either None (will return the default network input shape),\n",
    "            or a user-provided shape to be validated.\n",
    "        default_size: Default input width/height for the model.\n",
    "        min_size: Minimum input width/height accepted by the model.\n",
    "        data_format: Image data format to use.\n",
    "        require_flatten: Whether the model is expected to\n",
    "            be linked to a classifier via a Flatten layer.\n",
    "        weights: One of `None` (random initialization)\n",
    "            or 'imagenet' (pre-training on ImageNet).\n",
    "            If weights='imagenet' input channels must be equal to 3.\n",
    "    # Returns\n",
    "        An integer shape tuple (may include None entries).\n",
    "    # Raises\n",
    "        ValueError: In case of invalid argument values.\n",
    "    \"\"\"\n",
    "    if weights != 'imagenet' and input_shape and len(input_shape) == 3:\n",
    "        if input_shape[-1] not in {1, 3}:\n",
    "            warnings.warn(\n",
    "                'This model usually expects 1 or 3 input channels. '\n",
    "                'However, it was passed an input_shape with ' +\n",
    "                str(input_shape[-1]) + ' input channels.')\n",
    "        default_shape = (default_size, default_size, input_shape[-1])\n",
    "    else:\n",
    "        default_shape = (default_size, default_size, 3)\n",
    "\n",
    "    if weights == 'imagenet' and require_flatten:\n",
    "        if input_shape is not None:\n",
    "            if input_shape != default_shape:\n",
    "                raise ValueError('When setting `include_top=True` '\n",
    "                                 'and loading `imagenet` weights, '\n",
    "                                 '`input_shape` should be ' +\n",
    "                                 str(default_shape) + '.')\n",
    "        return default_shape\n",
    "\n",
    "    if input_shape:\n",
    "        if input_shape is not None:\n",
    "            if len(input_shape) != 3:\n",
    "                raise ValueError(\n",
    "                    '`input_shape` must be a tuple of three integers.')\n",
    "            if input_shape[-1] != 3 and weights == 'imagenet':\n",
    "                raise ValueError('The input must have 3 channels; got '\n",
    "                                    '`input_shape=' + str(input_shape) + '`')\n",
    "            if ((input_shape[0] is not None and input_shape[0] < min_size) or\n",
    "                (input_shape[1] is not None and input_shape[1] < min_size)):\n",
    "                raise ValueError('Input size must be at least ' +\n",
    "                                    str(min_size) + 'x' + str(min_size) +\n",
    "                                    '; got `input_shape=' +\n",
    "                                    str(input_shape) + '`')\n",
    "    else:\n",
    "        if require_flatten:\n",
    "            input_shape = default_shape\n",
    "        else:\n",
    "            input_shape = (None, None, 3)\n",
    "\n",
    "    if require_flatten:\n",
    "        if None in input_shape:\n",
    "            raise ValueError('If `include_top` is True, '\n",
    "                             'you should specify a static `input_shape`. '\n",
    "                             'Got `input_shape=' + str(input_shape) + '`')\n",
    "    return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(style_file, dataset_path, weights_path, content_weight, style_weight, \n",
    "            tv_weight, learning_rate, batch_size, epochs, debug):\n",
    "\n",
    "    # Setup the given layers\n",
    "    content_layers = ['block4_conv2']\n",
    "\n",
    "    style_layers = ['block1_conv1',\n",
    "                    'block2_conv1',\n",
    "                    'block3_conv1',\n",
    "                    'block4_conv1',\n",
    "                    'block5_conv1']\n",
    "\n",
    "    # Build Feed-forward transformer\n",
    "    network = feed_forward()\n",
    "\n",
    "    # Build VGG-19 Loss network\n",
    "    extractor = StyleContentModel(style_layers, content_layers)\n",
    "\n",
    "    # Load style target image\n",
    "    style_image = load_img(style_file, resize=False)\n",
    "\n",
    "    # Initialize content target images\n",
    "    batch_shape = (batch_size, 256, 256, 3)\n",
    "    X_batch = np.zeros(batch_shape, dtype=np.float32)\n",
    "\n",
    "    # Extract style target \n",
    "    style_target = extractor(style_image*255.0)['style']\n",
    "\n",
    "    # Build optimizer\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    loss_metric = tf.keras.metrics.Mean()\n",
    "    sloss_metric = tf.keras.metrics.Mean()\n",
    "    closs_metric = tf.keras.metrics.Mean()\n",
    "    tloss_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "\n",
    "    @tf.function()\n",
    "    def train_step(X_batch):\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            content_target = extractor(X_batch*255.0)['content']\n",
    "            image = network(X_batch)\n",
    "            outputs = extractor(image)\n",
    "            \n",
    "            s_loss = style_weight * style_loss(outputs['style'], style_target)\n",
    "            c_loss = content_weight * content_loss(outputs['content'], content_target)\n",
    "            t_loss = tv_weight * total_variation_loss(image)\n",
    "            loss = s_loss + c_loss + t_loss\n",
    "\n",
    "        grad = tape.gradient(loss, network.trainable_variables)\n",
    "        opt.apply_gradients(zip(grad, network.trainable_variables))\n",
    "\n",
    "        loss_metric(loss)\n",
    "        sloss_metric(s_loss)\n",
    "        closs_metric(c_loss)\n",
    "        tloss_metric(t_loss)\n",
    "\n",
    "\n",
    "    train_dataset = tf.data.Dataset.list_files(dataset_path + '/*.jpg')\n",
    "    train_dataset = train_dataset.map(load_img,\n",
    "                                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    train_dataset = train_dataset.shuffle(1024)\n",
    "    train_dataset = train_dataset.batch(batch_size, drop_remainder=True)\n",
    "    train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    import time\n",
    "    start = time.time()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        print('Epoch {}'.format(e))\n",
    "        iteration = 0\n",
    "\n",
    "        for img in train_dataset:\n",
    "\n",
    "            for j, img_p in enumerate(img):\n",
    "                X_batch[j] = img_p\n",
    "\n",
    "            iteration += 1\n",
    "            \n",
    "            train_step(X_batch)\n",
    "\n",
    "            if iteration % 3000 == 0:\n",
    "                # Save checkpoints\n",
    "                network.save_weights(weights_path, save_format='tf')\n",
    "                print('=====================================')\n",
    "                print('            Weights saved!           ')\n",
    "                print('=====================================\\n')\n",
    "\n",
    "                if debug:\n",
    "                    print('step %s: loss = %s' % (iteration, loss_metric.result()))\n",
    "                    print('s_loss={}, c_loss={}, t_loss={}'.format(sloss_metric.result(), closs_metric.result(), tloss_metric.result()))\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Total time: {:.1f}\".format(end-start))\n",
    "    \n",
    "\n",
    "    # Training is done !\n",
    "    network.save_weights(weights_path, save_format='tf')\n",
    "    print('=====================================')\n",
    "    print('             All saved!              ')\n",
    "    print('=====================================\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e6cba64f92086cea4f7c8f7f244f42e1e8cb9624a24fb73ff9c983ad8555d3f6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('multitask-bert': virtualenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
